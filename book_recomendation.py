# -*- coding: utf-8 -*-
"""Book Recomendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uJF6S5p2AjqDmMCN_NOcw3duSxzFN6DZ

#Load Data
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
 
book = pd.read_csv("/content/drive/MyDrive/dataset/book/Books.csv")
rating = pd.read_csv("/content/drive/MyDrive/dataset/book/Ratings.csv")
user = pd.read_csv("/content/drive/MyDrive/dataset/book/Users.csv")
 
print('Jumlah buku: ', len(book.ISBN.unique()))
print('Jumlah rating: ', len(rating.ISBN.unique()))
print('Jumlah user: ', len(user))

"""#Data Understanding

##Univariate Exploratory Data Analysis

###Data buku
"""

book.info()

print('Jumlah Judul Buku: ', len(book['Book-Title'].unique()))
print('Judul Buku: ', book['Book-Title'].unique())

"""###Data Rating"""

rating.info()

rating.describe()

print('Jumlah ID user: ', len(rating['User-ID'].unique()))
print('Jumlah ISBN: ', len(rating['ISBN'].unique()))
print('Jumlah data penilaian buku: ', len(rating['Book-Rating']))

"""###Data User"""

user.info()

print('Jumlah user: ', len(user))

"""#Data Preparation

##Menangani missing value

###Data Buku
"""

book.isnull().sum()

book = book.dropna()
book

book.isnull().sum()

"""###Data rating"""

rating.isnull().sum()

"""###Data User"""

user.isnull().sum()

user = user.dropna()
user

user.isnull().sum()

"""##Menggabungkan data"""

all_book = pd.merge(rating, book[['ISBN','Book-Title','Book-Author']], on='ISBN', how='left')
all_book

all_book.isnull().sum()

all_book = all_book.dropna()
all_book

"""##Drop duplikat values"""

all_book = all_book.drop_duplicates('ISBN')
all_book

"""##Mengetahui jumlah rating"""

all_book.groupby('User-ID').sum()

"""##Konversi data menjadi list"""

book_id = all_book['ISBN'].tolist()

book_title = all_book['Book-Title'].tolist()

book_author = all_book['Book-Author'].tolist()

print(len(book_id))
print(len(book_title))
print(len(book_author))

"""##Membuat dictionary untuk menentukan pasangan key-value pada data"""

all_book_new = pd.DataFrame({
    'book_id': book_id,
    'book_title': book_title,
    'book_author': book_author
})
all_book_new

"""##Mengurangi jumlah data
karena data terlalu banyak menyebabkan resource yang dibutuhkan melebihi kapasitas yang disediakan secara gratis oleh google colab sebagai platform melatih model machine learning ini maka data harus dikurangi menjadi 10000 data saja
"""

all_book_new = all_book_new[:10000]
all_book_new

rating = rating[:10000]
rating

"""#Modelling

##Content Based Filtering
"""

data = all_book_new
data.sample(5)

"""##TF-IDF Vectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

tf.fit(data['book_title']) 

tf.get_feature_names()

tfidf_matrix = tf.fit_transform(data['book_title']) 
 
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.book_author
).sample(22, axis=1).sample(10, axis=0)

"""###Cosine Similarity"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_author'], columns=data['book_author'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""###Mendapatkan Rekomendasi"""

def book_recommendations(author, similarity_data=cosine_sim_df, items=data[['book_title', 'book_author']], k=5):

    index = similarity_data.loc[:,author].to_numpy().argpartition(
        range(-1, -k, -1))
    
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    closest = closest.drop(author, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

data[data.book_author.eq('Jill McCorkle')]

book_recommendations('Jill McCorkle')

"""##Collaborative Filtering

###Data Understanding
"""

import pandas as pd
import numpy as np 
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

df = rating
df

"""###Data Preparation

####Encode
"""

user_ids = df['User-ID'].unique().tolist()
print('list User-ID: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded User-ID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke User-ID: ', user_encoded_to_user)

ISBN_ids = df['ISBN'].unique().tolist()

ISBN_to_ISBN_encoded = {x: i for i, x in enumerate(ISBN_ids)}
 
ISBN_encoded_to_ISBN = {i: x for i, x in enumerate(ISBN_ids)}

"""####Mapping kedalam dataframe"""

df['user'] = df['User-ID'].map(user_to_user_encoded)
 
df['ISBN'] = df['ISBN'].map(ISBN_to_ISBN_encoded)

num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah resto
num_ISBN = len(ISBN_encoded_to_ISBN)
print(num_ISBN)
 
# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['Book-Rating'])
 
# Nilai maksimal rating
max_rating = max(df['Book-Rating'])
 
print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_ISBN, min_rating, max_rating
))

"""####Membagi Data untuk Training dan Validasi"""

df = df.sample(frac=1, random_state=42)
df

x = df[['user', 'ISBN']].values
 
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""###Proses Training"""

class RecommenderNet(tf.keras.Model):
 
  def __init__(self, num_users, num_ISBN, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_ISBN = num_ISBN
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.ISBN_embedding = layers.Embedding(
        num_ISBN,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.ISBN_bias = layers.Embedding(num_ISBN, 1)
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    ISBN_vector = self.ISBN_embedding(inputs[:, 1])
    ISBN_bias = self.ISBN_bias(inputs[:, 1])
 
    dot_user_ISBN = tf.tensordot(user_vector, ISBN_vector, 2) 
 
    x = dot_user_ISBN + user_bias + ISBN_bias
    
    return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_ISBN, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""###Visualisasi Metrik"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""###Mendapatkan Rekomendasi"""

book_df = all_book_new
df = pd.read_csv('/content/drive/MyDrive/dataset/book/Ratings.csv')
df = df[:10000]

user_id = df['User-ID'].sample(1).iloc[0]
book_visited_by_user = df[df['User-ID'] == user_id]

book_not_visited = book_df[~book_df['book_id'].isin(book_visited_by_user.ISBN.values)]['book_id'] 
book_not_visited = list(
    set(book_not_visited)
    .intersection(set(ISBN_to_ISBN_encoded.keys()))
)
 
book_not_visited = [[ISBN_to_ISBN_encoded.get(x)] for x in book_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_visited), book_not_visited)
)

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    ISBN_encoded_to_ISBN.get(book_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)
 
top_book_user = (
    book_visited_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)
 
book_df_rows = book_df[book_df['book_id'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_author, ':', row.book_title)
 
print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)
 
recommended_book = book_df[book_df['book_id'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.book_author, ':', row.book_title)